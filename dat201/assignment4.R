#| *************************************************************************
#| *                                                                       *
#| *                          ASSIGNMENT #4                                *
#| *                                                                       *
#| *************************************************************************
#|
#| All of my comments are prepended with the pipe character

# Please practice the code and explain
# 1. every single results
# 2. every argument/parameters
# 3. Explain the followings - 

#| https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/

# what is Confusion Matrix
#| A confusion matrix visualizes and summarizes the performance of a 
#| classification algorithm.  It compares the number of predicted values and 
#| the true actual values.  There are 4 basic metrics:
#|    True Positives (TP): predicted positive and correct
#|    True Negatives (TN): predicted negative and correct
#|    False Positives (FP): predicted positive but incorrect
#|    False Negatives (FN): predicted negative but incorrect
#| For example:
#|             Predicted 
#|              P    N
#|   Real  P   TP   FN
#|         N   FP   TN

# how to get Accuracy - waht does it gives
#| ACCURACY = (TP + TN) / (TP + TN + FP + FN)
#| Overall, how often is the prediction model correct?

# what does "No Information Rate" means - why do we need it
#| The largest proportion of the observed classes. In the example below, the 
#| value 0.6095 indicates there are more "n" classes than "y" in the training
#| data set.

# what is Kappa? What does it measures
#| How well the classifier performed as compared to how well it would have 
#| performed simply by chance.  

# what does Mcnemar's test do here in the classification/decission tree results
#| The McNemar’s test is checking if the disagreements between two cases match. 
#| Technically, this is referred to as the homogeneity of the contingency table 
#| (specifically the marginal homogeneity). Therefore, the McNemar’s test is a 
#| type of homogeneity test for contingency tables.
#|    e.g. used in medicine to compare the effect of a treatment against a control.
#|    p > alpha: fail to reject H0, no difference in the disagreement 
#|      (e.g. treatment had no effect).
#|    p <= alpha: reject H0, significant difference in the disagreement 
#|      (e.g. treatment had an effect).

# what is "        Sensitivity  " - why do we need it.
#| SENSITIVITY = TP / (TP + FN)
#| Also known as: recall, hit rate, true positive rate
#| When the real value is positive, how often does it predict positive?
#| A good metric to use when we are focusing on limiting our false negatives.
#| e.g. diagnosing COVID-19; want to avoid people spreading the disease with a 
#| negative test.

# what is "         Specificity " - why do we need it.     
#| SPECIFICITY = TN / (TN + FP)
#| Also known as: selectivity, true negative rate
#| When the real value is negative, how often does it predict negative?

# what is "      Pos Pred Value " - why do we need it.     
#| POSITIVE PREDICTIVE VALUE (PPV) = TP / (TP + FP)
#| Also called: precision
#| A good measure to use when we want to focus on limiting false positives. 

# what is "      Neg Pred Value " - why do we need it.
#| NEGATIVE PREDICTIVE VALUE (NPV) = TN / (TN + FN)
#| 

# what is "          Prevalence " - why do we need it.     
#| PREVALENCE = P / P+N
#| How often does the positive condition actually occur in our sample?

# what is "      Detection Rate " - why do we need it.     
#| DETECTION RATE = TP / (TP + TN + FP + FN)

# what is "Detection Prevalence " - why do we need it.     
#| DETECTION PREVALENCE = (TP + FP) / (TP + TN + FP + FN)

# what does predict function do?
#| When given an object of class 'tree' as the first argument, such as one 
#| created by the function rpart(), it returns a vector of predicted responses 
#| from the fitted tree object.

# What is ROC - why does it do and why do we need it
#| ROC stands for "receiver operating characteristic (ROC) curve".  A visual 
#| method to measure the performance of a binary classifier.  
#| It is generated by plotting the True Positive Rate (Sensitivity) against the 
#| False Positive Rate (Specificity) as you vary the threshold for assigning 
#| observations to a given class. 
#| The area under that curve (AUC) is a way to summarize the performance using
#| a single number.



# https://www.r-bloggers.com/2021/04/decision-trees-in-r/
# https://www.guru99.com/r-decision-trees.html


#install.packages(c("DAAG", "party", "rpart", "rpart.plot", "mlbench", "caret", "pROC", "tree"  ))

# Load Library

library(DAAG)
library(party)
library(rpart)
library(rpart.plot)
library(mlbench)
library(caret)
library(pROC)
library(tree)

# Getting Data -Email Spam Detection

str(spam7) 
# data.frame':  4601 obs. of  7 variables:
#  $ crl.tot: num  278 1028 2259 191 191 ...
#  $ dollar : num  0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ...
#  $ bang   : num  0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ...
#  $ money  : num  0 0.43 0.06 0 0 0 0 0 0.15 0 ...
#  $ n000   : num  0 0.43 1.16 0 0 0 0 0 0 0.19 ...
#  $ make   : num  0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...
#  $ yesno  : Factor w/ 2 levels "n","y": 2 2 2 2 2 2 2 2 2 2 ...

# Total 4601 observations and 7 variables.

# data description
?spam7

mydata <- spam7
 
# Data Partition
#| Separate our original data sample into two parts: training and testing.
#| The training data will be used to determine the details of the model (i.e.
#| the coefficients, branches, etc.)  The test data will be used to check the
#| performance (i.e. accuracy, recall, etc.) This is a good way to test since 
#| the model has never seen the test before and is not influcenced by it. 
set.seed(1234)
ind <- sample(2, nrow(mydata), replace = T, prob = c(0.5, 0.5))
train <- mydata[ind == 1,]
test <- mydata[ind == 2,]

# Tree Classification
?rpart # algorithm that creates the dicisions
?rpart.object # algorithm that creates the dicisions

#| The next 2 lines creates a new classification model using the training data
#| and plots a diagram of it.  
#| For example, the 2nd node along the left branch of the root ("n, 0.23, 75%")
#| show the following:
#|    1. n -> The prediction at this point is "not spam"
#|    2. 0.23 -> there is a 23% chance of being spam at this point in the tree
#|    3. 75% -> the proportion of the data that has <5.6% dollar sign 
#|              characters (decision from the previous node)

tree <- rpart(yesno ~., data = train)
rpart.plot(tree)

#| cp is the "complexity paramater".  rpart() uses it to determine a threshold
#| value of when to attempt a split.  Any split that does not decrease the 
#| overall lack of fit by a factor of cp is not attempted.  This is to save 
#| computing time by pruning off splits that are obviously not worthwhile.

#| Prints a table of optimal prunings based on a complexity parameter.  
printcp(tree)
# Classification tree:
#   rpart(formula = yesno ~ ., data = train)
# Variables actually used in tree construction:
#   [1] bang    crl.tot dollar
# Root node error: 900/2305 = 0.39046
# n= 2305
#         CP nsplit rel error  xerror     xstd
# 1 0.474444      0   1.00000 1.00000 0.026024
# 2 0.074444      1   0.52556 0.56556 0.022128
# 3 0.010000      3   0.37667 0.42111 0.019773

#| A visual representation of the same information given by printcp(). The mean 
#| and standard deviation of the errors in the cross-validated prediction 
#| against each of the geometric means, and these are plotted by this function.
#| A good choice of cp for pruning is often the leftmost value for which the 
#| mean lies below the horizontal line.
plotcp(tree)

# https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf
# – cp: The threshold complexity parameter (you realy have to
#understand the theory to fully understand what is going on behind the seen)

# You can change the cp value according to your data set. 
# Please note lower cp value means bigger the tree. 
#If you are using too lower cp that leads to overfitting also.
#

#| Create another model with a different value for cp
tree <- rpart(yesno ~., data = train,cp=0.07444)

# Confusion matrix -train

#| Use the testing data, which is a portion of our original dataset that the 
#| model has never seen yet, to predict the outcome for each. 
p <- predict(tree, train, type = 'class')

#| Then, by creating a confusion model, we can evaluate the performance of this 
#| model. 
confusionMatrix(p, train$yesno, positive="y")

#| At least that's what SHOULD happen.  Why are the above lines using the train
#| data?  It should be the following, no?
p <- predict(tree, test, type = 'class')
confusionMatrix(p, test$yesno, positive="y")

#Please make sure you mention positive classes in the confusion matrix.
# 
#  Confusion Matrix and Statistics
#  Reference
#  Prediction    n    y
#  n 1278  212
#  y  127  688
#  Accuracy : 0.8529         
#  95% CI : (0.8378, 0.8671)
#  No Information Rate : 0.6095         
#  P-Value [Acc > NIR] : < 2.2e-16      
#  Kappa : 0.6857         
#  Mcnemar's Test P-Value : 5.061e-06      
#             Sensitivity : 0.7644         
#             Specificity : 0.9096         
#          Pos Pred Value : 0.8442         
#          Neg Pred Value : 0.8577         
#              Prevalence : 0.3905         
#          Detection Rate : 0.2985         
#    Detection Prevalence : 0.3536         
#       Balanced Accuracy : 0.8370         
#        'Positive' Class : y
# Model has 85% accuracy
# 
#  

#| SPAM CLASSIFIER PERFORMANCE RESULTS:
#| The overall accuracy of this model is 85% which is pretty good.  
#| We would want to minimize the number of false positives.  Better to allow 
#| real spam into the user's inbox than to incorrectly remove legitimate email.
#| We can use Sensitivity to measure this:  
#|     76.44% -> not great but still better than a simple guess (50%)


#ROC
#| A model that shows a curve towards the top-left is considered better. As 
#| the curve gets closer to the diagonal, the performance degradese towards
#| beign a baseline random selection (i.e. flip a coin)
#| In this case, we are much closer to the top-left corner than we are to the 
#| diagonal line, indicating this is model is a good fit.  
p1 <- predict(tree, test, type = 'prob')
p1 <- p1[,2]
r <- multiclass.roc(test$yesno, p1, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,
        print.auc=TRUE,
        auc.polygon=TRUE,
        grid=c(0.1, 0.2),
        grid.col=c("green", "red"),
        max.auc.polygon=TRUE,
        auc.polygon.col="lightblue",
        print.thres=TRUE,
        main= 'ROC Curve')


# Method 2- Regression  Tree

data('BostonHousing')
mydata <- BostonHousing
 

#Data Partition
#| As above, separate our data sample into two mutually exclusive groups:
#| training data (used to create our model) and testing data (used to evaluate
#| our model's performance)
set.seed(1234)
ind <- sample(2, nrow(mydata), replace = T, prob = c(0.5, 0.5))
train <- mydata[ind == 1,]
test <- mydata[ind == 2,]

#Regression tree
#| Create a model using the 'train' data set and plot a diagram of it.
tree <- rpart(medv ~., data = train)
rpart.plot(tree)


printcp(tree)
# Regression tree:
#   rpart(formula = medv ~ ., data = train)
# Variables actually used in tree construction:
#   [1] age   crim  lstat rm  
# Root node error: 22620/262 = 86.334
# n= 262
# CP nsplit rel error  xerror     xstd
# 0.469231      0   1.00000 1.01139 0.115186
# 2 0.128700      1   0.53077 0.62346 0.080154
# 3 0.098630      2   0.40207 0.51042 0.076055
# 4 0.033799      3   0.30344 0.42674 0.069827
# 5 0.028885      4   0.26964 0.39232 0.066342
# 6 0.028018      5   0.24075 0.37848 0.066389
# 7 0.015141      6   0.21274 0.34877 0.065824
# 8 0.010000      7   0.19760 0.33707 0.065641

#| Similar to the tree diagram above, we can also see the decision rules of
#| of each node in the tree as a list
rpart.rules(tree)
#medv                                                                       
# 13 when lstat >=        14.8 & crim >= 5.8   
# 17 when lstat >=        14.8 & crim <  5.8    
# 22 when lstat is 7.2 to 14.8 & rm <  6.6                                    
# 26 when lstat <  7.2         & rm <  6.8        & age <  89                 
# 29 when lstat is 7.2 to 14.8 & rm >=        6.6                             
# 33 when lstat <  7.2         & rm is 6.8 to 7.5 & age <  89                 
# 40 when lstat <  7.2         & rm <  7.5        & age >= 89                 
# 45 when lstat <  7.2         & rm >=        7.5       

#| The leftmost value where the mean is below the horizontal line (0.4) is 
#| 0.031.  It would be worth creating another model with cp=0.031 and compare
#| performance of each.
plotcp(tree)

 
 
# Predict
#| Not sure why we aren't using 'test' data here.  We should ealuate the 
#| performance of the model using data not used in the creation of it.  
p <- predict(tree, train)

# Root Mean Square Error
#| Corresponds to the average difference between the observed known values of 
#| the outcome and the predicted value by the model.  Lower is better.
sqrt(mean((train$medv-p)^2))
#4.130294
 
# R Square
#| Explains the amount of variation in the response variable ('medv' in this 
#| case) that is due to variation in the feature variables as a percentage.  
#| A higher number indicates a good fit for the model.
(cor(train$medv,p))^2
#0.8024039
 
# Conclusion
# In the regression model, the r square value is 80% 
# and RMSE is 4.13, not bad at all.
# .In this way, you can make use of 
# Decision classification regression tree models.
# 

